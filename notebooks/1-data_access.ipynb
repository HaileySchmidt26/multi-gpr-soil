{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Access and Cleaning\n",
    "\n",
    "The purpose of this notebook is to download the subsurface data from the Washington State Department of Natural Resources (DNR) [Geologic Information Portal](https://www.dnr.wa.gov/geologyportal), available for [download](https://www.dnr.wa.gov/programs-and-services/geology/publications-and-data/gis-data-and-databases). \n",
    "\n",
    "This data is then filtered to our desired area of interest, clipped to the extent of the study area, and saved as a GeoPackage file.\n",
    "\n",
    "Finally, the data is loaded for exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL and save path for the zip file\n",
    "url = 'https://fortress.wa.gov/dnr/geologydata/publications/data_download/ger_portal_subsurface_database.zip'\n",
    "save_path = '../data/temp/ger_portal_subsurface_database.zip'\n",
    "extract_path = '../data/temp/extracted_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the target folders exist\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "os.makedirs(extract_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded file saved to ../data/temp/ger_portal_subsurface_database.zip\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with requests.get(url, stream=True) as response:\n",
    "        response.raise_for_status()\n",
    "        with open(save_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):  # Download in chunks\n",
    "                file.write(chunk)\n",
    "    print(f\"Downloaded file saved to {save_path}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Failed to download the file. Error: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files extracted to ../data/temp/extracted_files/\n"
     ]
    }
   ],
   "source": [
    "# Check if the file is a valid ZIP file and extract files\n",
    "if zipfile.is_zipfile(save_path):\n",
    "    try:\n",
    "        with zipfile.ZipFile(save_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "        print(f\"Files extracted to {extract_path}\")\n",
    "    except zipfile.BadZipFile as e:\n",
    "        print(f\"Error: The file at {save_path} is a bad ZIP file. {str(e)}\")\n",
    "else:\n",
    "    print(f\"Error: The file at {save_path} is not recognized as a valid ZIP file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter and clip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted files: ['README_subsurface.docx', 'WGS_Subsurface_Database.gdb', 'metadata', 'Subsurface_Database.mpkx', 'layer_files']\n"
     ]
    }
   ],
   "source": [
    "# View the extracted files\n",
    "extracted_files = os.listdir(extract_path)\n",
    "print(f\"Extracted files: {extracted_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers in the geodatabase: ['bedrock', 'hydrologic', 'layer_comments', 'documents', 'in_situ_test_data', 'borehole_info', 'layer_data', 'lithology_logs', 'water_wells', 'subsurface_documents']\n"
     ]
    }
   ],
   "source": [
    "# Open the geodatabase file: 'WGS_Subsurface_Database.gdb' in the extracted files\n",
    "gdb_path = os.path.join(extract_path, 'WGS_Subsurface_Database.gdb') \n",
    "\n",
    "# List all layers in the geodatabase\n",
    "layers = fiona.listlayers(gdb_path)\n",
    "print(f\"Layers in the geodatabase: {layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are interested in the layer_data layer table\n",
    "layer_data = gpd.read_file(gdb_path, layer='layer_data')\n",
    "print(f\"Shape of the layer_data table: {layer_data.shape}\")\n",
    "\n",
    "print(\"Columns:\", layer_data.columns)\n",
    "\n",
    "# Display the first few rows of the layer_data table\n",
    "layer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the columns of interest\n",
    "columns_of_interest = ['BOREHOLE_ID', 'LAYER_NUMBER', 'TOP_DEPTH_FT', 'BOTTOM_DEPTH_FT', 'USCS']\n",
    "\n",
    "# Filter the layer_data table to only include the columns of interest\n",
    "layer_data_filtered = layer_data[columns_of_interest]\n",
    "\n",
    "# Display the first few rows of the filtered table\n",
    "layer_data_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a column of \"simplified USCS\" for our purposes\n",
    "layer_data_filtered['SIMPLE_USCS'] = layer_data_filtered['USCS'].str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also read the 'borehole_info' table, which contains information about the boreholes such as location, depth, etc.\n",
    "borehole_info = gpd.read_file(gdb_path, layer='borehole_info')\n",
    "print(f\"Shape of the borehole_info table: {borehole_info.shape}\")\n",
    "\n",
    "print(\"Columns:\", borehole_info.columns)\n",
    "\n",
    "# Display the first few rows of the borehole_info table\n",
    "borehole_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the columns of interest\n",
    "columns_of_interest = ['BOREHOLE_ID', 'BOREHOLE_NAME', 'BOREHOLE_TYPE', 'BOREHOLE_DEPTH_FT', 'ELEVATION_FT', 'LATITUDE', 'LONGITUDE']\n",
    "\n",
    "# Filter the borehole_info table to only include the columns of interest\n",
    "borehole_info_filtered = borehole_info[columns_of_interest]\n",
    "\n",
    "# Display the first few rows of the filtered table\n",
    "borehole_info_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two tables based on the 'BOREHOLE_ID' column, note that there are multiple layer_data entries for each borehole\n",
    "borehole_data = pd.merge(borehole_info_filtered, layer_data_filtered, on='BOREHOLE_ID', how='inner')\n",
    "print(f\"Shape of the merged table: {borehole_data.shape}\")\n",
    "\n",
    "# Display the first few rows of the merged table\n",
    "borehole_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are only interested in 'Geotechnical' borehole types\n",
    "borehole_data = borehole_data[borehole_data['BOREHOLE_TYPE'] == 'Geotechnical']\n",
    "\n",
    "# Display the first few rows of the filtered table\n",
    "borehole_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with \"Unknown\" or \"Not Applicable\" values in the 'USCS' column\n",
    "# borehole_data = borehole_data[(borehole_data['USCS'] != 'Unknown') & ('Not Applicable' not in borehole_data['USCS']) & ('Not applicable' not in borehole_data['USCS']) & ('Now applicable' not in borehole_data['USCS']) & ('Not applicabble' not in borehole_data['USCS']) & ('Not application' not in borehole_data['USCS'])]\n",
    "# print(f\"Shape of the filtered table: {borehole_data.shape}\")\n",
    "# Define the regex pattern to match variations of \"Unknown\" and \"Not Applicable\"\n",
    "pattern = r'Unknown|Not\\s*Applicable|Not\\s*applicable|Now\\s*applicable|Not\\s*applicabble|Not\\s*application|Non-standard'\n",
    "\n",
    "# Drop rows where the 'USCS' column contains any of the specified patterns\n",
    "borehole_data = borehole_data[~borehole_data['USCS'].str.contains(pattern, case=False, na=False)]\n",
    "\n",
    "# Compute the thickness of each layer\n",
    "borehole_data['LAYER_THICKNESS_FT'] = borehole_data['BOTTOM_DEPTH_FT'] - borehole_data['TOP_DEPTH_FT']\n",
    "\n",
    "# Display the first few rows of the table with the thickness column\n",
    "borehole_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the final table to a CSV file\n",
    "output_file = '../data/0-borehole_data.csv'\n",
    "borehole_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the first iteration of this project, we will only try to predict the USCS and layer thickness of the uppermost soil layer (Layer 1).\n",
    "# Read in the CSV file\n",
    "borehole_data = pd.read_csv(output_file)\n",
    "\n",
    "# Filter the data to only include Layer 1\n",
    "layer_1_data = borehole_data[borehole_data['LAYER_NUMBER'] == 1]\n",
    "\n",
    "# Display the first few rows of the filtered table\n",
    "layer_1_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are also only interested in a small region around the University of Washington, Seattle, to begin\n",
    "# Define the bounding box for the region around the University of Washington, Seattle\n",
    "min_lon, max_lon = -122.35, -122.3\n",
    "min_lat, max_lat = 47.65, 47.7\n",
    "\n",
    "# Filter the data to only include the region around the University of Washington, Seattle\n",
    "uw_layer1_data = layer_1_data[(layer_1_data['LONGITUDE'] >= min_lon) & (layer_1_data['LONGITUDE'] <= max_lon) & (layer_1_data['LATITUDE'] >= min_lat) & (layer_1_data['LATITUDE'] <= max_lat)]\n",
    "\n",
    "# Display the first few rows of the filtered table\n",
    "uw_layer1_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1 statistics\n",
    "uw_layer1_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the unique values in the 'USCS' column and their counts\n",
    "uscs_counts = uw_layer1_data['USCS'].value_counts()\n",
    "print(uscs_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the unique values in the 'USCS' column and their counts\n",
    "uscs_counts = uw_layer1_data['SIMPLE_USCS'].value_counts()\n",
    "print(uscs_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a histogram of the layer thickness values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(uw_layer1_data['LAYER_THICKNESS_FT'], bins=20, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the filtered data to a new CSV file\n",
    "output_file = '../data/1-uw_layer1_data.csv'\n",
    "uw_layer1_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: \n",
    "1. Washington Geological Survey, 2023, Subsurface database--GIS data, July 2023: Washington Geological Survey Digital Data Series 11, version 2.3, previously released March 2023. https://fortress.wa.gov/dnr/geologydata/publications/data_download/ger_portal_subsurface_database.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
